{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f95d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, copy, math\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision\n",
    "from torchvision import models, datasets\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, \n",
    "    confusion_matrix, classification_report, f1_score\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f24eaf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbfc33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录: e:\\1.code\\Jupyter-notebook\\MUST-DataScience\\1-groupwork\n",
      "数据增强目录: e:\\1.code\\Jupyter-notebook\\MUST-DataScience\\1-groupwork\\dataprocess\\augmented\n",
      "模型保存目录: e:\\1.code\\Jupyter-notebook\\MUST-DataScience\\1-groupwork\\cnn_models\\checkpoints\n",
      "模型结果目录: e:\\1.code\\Jupyter-notebook\\MUST-DataScience\\1-groupwork\\cnn_models\\results\n",
      "类别数: 8\n"
     ]
    }
   ],
   "source": [
    "work_dir = os.path.abspath(\"..\")  \n",
    "data_dir = os.path.join(work_dir, \"dataprocess\", \"augmented\")   # 你的增强数据\n",
    "save_dir = os.path.join(work_dir, \"Mobilenetv3-Large\",\"checkpoints\") # 模型保存路径\n",
    "result_dir = os.path.join(work_dir,\"Mobilenetv3-Large\",\"results\")  # 结果保存路径\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print(\"当前工作目录:\", work_dir)\n",
    "print(\"数据增强目录:\", data_dir)\n",
    "print(\"模型保存目录:\", save_dir)\n",
    "print(\"模型结果目录:\", result_dir)\n",
    "\n",
    "num_classes = len([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir,d))])\n",
    "print(\"类别数:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2770d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    \"model_name\": \"mobilenet_v3_large\",\n",
    "    \"img_size\": 224,\n",
    "    \"batch_size\": 32,\n",
    "    \"stage1_epochs\": 10,\n",
    "    \"stage2_epochs\": 15,\n",
    "    \"lr_stage1\": 1e-3,\n",
    "    \"lr_stage2\": 1e-5,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"num_workers\": 4,\n",
    "    \"save_path\": save_dir,\n",
    "    \"accumulation_steps\": 1,\n",
    "    \"early_stopping_patience\": 8,\n",
    "    \"use_mixup\": True,\n",
    "    \"mixup_alpha\": 0.2,\n",
    "    \"n_splits\": 5,  # K折数量：推荐5-10折\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a803f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data trasnsforms\n",
    "from torchvision import datasets, transforms\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),#水平翻转\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),#增强颜色变化\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),# 平移\n",
    "        transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=0),#随即遮挡图片区域，用黑色填充\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "819a36ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.2, device=None):\n",
    "    if alpha <= 0:\n",
    "        return x, y, None, 1.0\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Mixup损失函数\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc21da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device, \n",
    "                use_mixup=True, mixup_alpha=0.2):\n",
    "    \"\"\"训练一个epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_mixup:\n",
    "            images, labels_a, labels_b, lam = mixup_data(\n",
    "                images, labels, alpha=mixup_alpha\n",
    "            )\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        if use_mixup:\n",
    "            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            if lam >= 0.5:\n",
    "                running_corrects += torch.sum(preds == labels_a.data)\n",
    "            else:\n",
    "                running_corrects += torch.sum(preds == labels_b.data)\n",
    "        else:\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix({'loss': running_loss / total_samples})\n",
    "    \n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects.double() / total_samples\n",
    "    \n",
    "    return epoch_loss, epoch_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd6f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"验证一个epoch\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            pbar.set_postfix({'loss': running_loss / total_samples})\n",
    "    \n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects.double() / total_samples\n",
    "    \n",
    "    return epoch_loss, epoch_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c5c44a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.0005, save_path=\"best_model.pth\", best_model_path=None):\n",
    "        \"\"\"\n",
    "        patience：多少 epoch 内没有提升就停止\n",
    "        delta：最小提升幅度\n",
    "        save_path：最优模型保存路径\n",
    "        best_model_path：最优模型保存路径（兼容参数）\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.delta = delta\n",
    "        self.save_path = best_model_path if best_model_path else save_path\n",
    "        self.best_model_path = self.save_path\n",
    "        self.early_stop = False\n",
    "        self.best_epoch=0\n",
    "\n",
    "    def __call__(self, val_loss, model,epoch):\n",
    "        # 第一次直接保存\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model, epoch)\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter =0\n",
    "            self.save_checkpoint(model, epoch)\n",
    "\n",
    "        return self.counter >= self.patience\n",
    "    \n",
    "    def save_checkpoint(self, model, epoch):\n",
    "        torch.save(model.state_dict(), self.best_model_path)\n",
    "        self.best_epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff50fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes,device):\n",
    "    from torchvision.models import mobilenet_v3_large\n",
    "    from torchvision.models import MobileNet_V3_Large_Weights\n",
    "\n",
    "    # 加载预训练模型\n",
    "    weight = MobileNet_V3_Large_Weights.DEFAULT\n",
    "    model = mobilenet_v3_large(weights=weight)\n",
    "\n",
    "    # 冻结backbone权重\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False #冻结梯度 \n",
    "\n",
    "    # 修改分类头 别的不做修改\n",
    "    in_features = model.classifier[3].in_features\n",
    "    model.classifier[3] = nn.Linear(in_features, num_classes)\n",
    "    print(model)\n",
    "\n",
    "    in_features = model.classifier[0].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.BatchNorm1d(256),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "    \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2aae9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_two_stage(model, train_loader, val_loader, device, CFG, fold_num=0):\n",
    "    \"\"\"两步走训练策略\"\"\"\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    total_epochs = CFG[\"stage1_epochs\"] + CFG[\"stage2_epochs\"]\n",
    "    \n",
    "    # ========== 阶段1 ==========\n",
    "    print(f\"\\n[Fold {fold_num}] 阶段1: 训练分类头\")\n",
    "    \n",
    "    optimizer1 = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=CFG[\"lr_stage1\"],\n",
    "        weight_decay=CFG[\"weight_decay\"]\n",
    "    )\n",
    "    \n",
    "    scheduler1 = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer1, T_max=CFG[\"stage1_epochs\"]\n",
    "    )\n",
    "    \n",
    "    early_stopping1 = EarlyStopping(\n",
    "        patience=CFG[\"early_stopping_patience\"],\n",
    "        best_model_path=os.path.join(CFG[\"save_path\"], f\"fold{fold_num}_stage1.pth\")\n",
    "    )\n",
    "    \n",
    "    for epoch in range(CFG[\"stage1_epochs\"]):\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer1, device,\n",
    "            use_mixup=CFG[\"use_mixup\"], mixup_alpha=CFG[\"mixup_alpha\"]\n",
    "        )\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        scheduler1.step()\n",
    "        current_lr = optimizer1.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"  Epoch [{epoch+1}/{CFG['stage1_epochs']}] \"\n",
    "              f\"TL: {train_loss:.4f} TA: {train_acc:.4f} \"\n",
    "              f\"VL: {val_loss:.4f} VA: {val_acc:.4f} \"\n",
    "              f\"LR: {current_lr:.6f}\")\n",
    "        \n",
    "        if early_stopping1(val_loss, model, epoch):\n",
    "            print(f\"   早停 (epoch {epoch+1})\")\n",
    "            break\n",
    "    \n",
    "    # ========== 阶段2 ==========\n",
    "    print(f\"[Fold {fold_num}] 阶段2: 微调所有层\")\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    optimizer2 = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CFG[\"lr_stage2\"],\n",
    "        weight_decay=CFG[\"weight_decay\"]\n",
    "    )\n",
    "    \n",
    "    scheduler2 = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer2, mode='min', factor=0.5, patience=3, verbose=False, min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    early_stopping2 = EarlyStopping(\n",
    "        patience=CFG[\"early_stopping_patience\"],\n",
    "        best_model_path=os.path.join(CFG[\"save_path\"], f\"fold{fold_num}_best.pth\")\n",
    "    )\n",
    "    \n",
    "    for epoch in range(CFG[\"stage2_epochs\"]):\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer2, device,\n",
    "            use_mixup=CFG[\"use_mixup\"], mixup_alpha=CFG[\"mixup_alpha\"]\n",
    "        )\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        scheduler2.step(val_loss)\n",
    "        current_lr = optimizer2.param_groups[0]['lr']\n",
    "        \n",
    "        print(f\"  Epoch [{CFG['stage1_epochs']+epoch+1}/{total_epochs}] \"\n",
    "              f\"TL: {train_loss:.4f} TA: {train_acc:.4f} \"\n",
    "              f\"VL: {val_loss:.4f} VA: {val_acc:.4f} \"\n",
    "              f\"LR: {current_lr:.6f}\")\n",
    "        \n",
    "        if early_stopping2(val_loss, model, epoch):\n",
    "            print(f\"   早停 (epoch {CFG['stage1_epochs'] + epoch+1})\")\n",
    "            break\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0f9709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stratified_kfold_validation(data_dir, num_classes, device, CFG):\n",
    "    \"\"\"\n",
    "    运行分层K折交叉验证\n",
    "    关键特性：\n",
    "    1. 保证每个fold的类别分布一致\n",
    "    2. 完整的两阶段训练\n",
    "    3. 详细的性能评估\n",
    "    4. 模型融合预测\n",
    "    \"\"\"\n",
    "    \n",
    "    # 加载完整数据集\n",
    "    full_dataset = datasets.ImageFolder(\n",
    "        root=data_dir,\n",
    "        transform=data_transforms[\"train\"]\n",
    "    )\n",
    "    \n",
    "    class_names = full_dataset.classes\n",
    "    print(f\" 加载数据集: {len(full_dataset)} 样本, {num_classes} 类别\")\n",
    "    \n",
    "    # 获取所有标签\n",
    "    all_labels = [full_dataset[i][1] for i in range(len(full_dataset))]\n",
    "    all_indices = np.arange(len(full_dataset))\n",
    "    \n",
    "    # 初始化分层K折\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=CFG[\"n_splits\"],\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    fold_results = []\n",
    "    all_fold_preds = defaultdict(list)\n",
    "    all_fold_labels = defaultdict(list)\n",
    "    all_fold_probs = defaultdict(list)\n",
    "    \n",
    "    # ========= K折循环 =========\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(all_indices, all_labels)):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Fold {fold+1}/{CFG['n_splits']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # 创建子集\n",
    "        train_subset = Subset(full_dataset, train_idx)\n",
    "        val_subset = Subset(full_dataset, val_idx)\n",
    "        \n",
    "        # 为验证集设置不同的transform\n",
    "        val_subset_with_transform = copy.deepcopy(val_subset)\n",
    "        val_subset_with_transform.dataset.transform = data_transforms[\"val\"]\n",
    "        \n",
    "        # 创建数据加载器\n",
    "        train_loader = DataLoader(\n",
    "            train_subset, batch_size=CFG[\"batch_size\"],\n",
    "            shuffle=True, num_workers=CFG[\"num_workers\"], pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_subset_with_transform, batch_size=CFG[\"batch_size\"],\n",
    "            shuffle=False, num_workers=CFG[\"num_workers\"], pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # 显示类别分布\n",
    "        fold_train_labels = [all_labels[i] for i in train_idx]\n",
    "        fold_val_labels = [all_labels[i] for i in val_idx]\n",
    "        \n",
    "        print(f\"训练: {len(train_idx)}, 验证: {len(val_idx)}\")\n",
    "        print(f\"训练集类别: {Counter(fold_train_labels)}\")\n",
    "        print(f\"验证集类别: {Counter(fold_val_labels)}\")\n",
    "        \n",
    "        # 创建新模型\n",
    "        model = create_model(num_classes, device)\n",
    "        \n",
    "        # 训练\n",
    "        history = train_two_stage(model, train_loader, val_loader, device, CFG, fold_num=fold+1)\n",
    "        \n",
    "        # 加载最优模型\n",
    "        best_model_path = os.path.join(CFG[\"save_path\"], f\"fold{fold+1}_best.pth\")\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        \n",
    "        # 在验证集上评估\n",
    "        model.eval()\n",
    "        fold_preds = []\n",
    "        fold_labels = []\n",
    "        fold_probs = []\n",
    "        fold_acc = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=\"Evaluating\", leave=False):\n",
    "                images = images.to(device)\n",
    "                outputs = model(images)\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                fold_preds.extend(preds.cpu().numpy())\n",
    "                fold_labels.extend(labels.numpy())\n",
    "                fold_probs.extend(probs.cpu().numpy())\n",
    "                fold_acc += torch.sum(preds == labels.to(device)).item()\n",
    "        \n",
    "        fold_acc = fold_acc / len(val_idx)\n",
    "        fold_f1 = f1_score(fold_labels, fold_preds, average='weighted')\n",
    "        \n",
    "        print(f\"\\n Fold {fold+1} 结果:\")\n",
    "        print(f\"   Accuracy: {fold_acc*100:.2f}%\")\n",
    "        print(f\"   Weighted F1: {fold_f1:.4f}\")\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'accuracy': fold_acc,\n",
    "            'f1_score': fold_f1,\n",
    "            'history': history,\n",
    "            'preds': np.array(fold_preds),\n",
    "            'labels': np.array(fold_labels),\n",
    "            'probs': np.array(fold_probs),\n",
    "            'train_idx': train_idx,\n",
    "            'val_idx': val_idx,\n",
    "            'model_path': best_model_path\n",
    "        })\n",
    "        \n",
    "        all_fold_preds[fold] = np.array(fold_preds)\n",
    "        all_fold_labels[fold] = np.array(fold_labels)\n",
    "        all_fold_probs[fold] = np.array(fold_probs)\n",
    "    \n",
    "    # ========= 整合结果 =========\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"K折交叉验证总结 (n_splits={CFG['n_splits']})\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    accuracies = [r['accuracy'] for r in fold_results]\n",
    "    f1_scores = [r['f1_score'] for r in fold_results]\n",
    "    \n",
    "    mean_acc = np.mean(accuracies)\n",
    "    std_acc = np.std(accuracies)\n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "    \n",
    "    print(f\"Accuracy: {mean_acc*100:.2f}% ± {std_acc*100:.2f}%\")\n",
    "    print(f\"各折: {[f'{acc*100:.2f}%' for acc in accuracies]}\")\n",
    "    print(f\"\\nWeighted F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "    print(f\"各折: {[f'{f1:.4f}' for f1 in f1_scores]}\")\n",
    "    \n",
    "    if std_acc < 0.02:\n",
    "        print(f\" 模型稳定性: 优秀 (std < 2%)\")\n",
    "    elif std_acc < 0.05:\n",
    "        print(f\" 模型稳定性: 良好 (std < 5%)\")\n",
    "    else:\n",
    "        print(f\" 模型稳定性: 一般\")\n",
    "    \n",
    "    return fold_results, class_names, all_fold_preds, all_fold_labels, all_fold_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9db33f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 加载数据集: 2860 样本, 8 类别\n",
      "\n",
      "======================================================================\n",
      "Fold 1/5\n",
      "======================================================================\n",
      "训练: 2288, 验证: 572\n",
      "训练集类别: Counter({2: 343, 3: 291, 5: 282, 0: 281, 1: 273, 6: 273, 7: 273, 4: 272})\n",
      "验证集类别: Counter({2: 86, 3: 72, 0: 71, 5: 70, 4: 69, 1: 68, 6: 68, 7: 68})\n",
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): Conv2dNormActivation(\n",
      "      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=960, out_features=1280, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=1280, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "[Fold 1] 阶段1: 训练分类头\n",
      "\n",
      "======================================================================\n",
      "Fold 1/5\n",
      "======================================================================\n",
      "训练: 2288, 验证: 572\n",
      "训练集类别: Counter({2: 343, 3: 291, 5: 282, 0: 281, 1: 273, 6: 273, 7: 273, 4: 272})\n",
      "验证集类别: Counter({2: 86, 3: 72, 0: 71, 5: 70, 4: 69, 1: 68, 6: 68, 7: 68})\n",
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): Conv2dNormActivation(\n",
      "      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=960, out_features=1280, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=1280, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "[Fold 1] 阶段1: 训练分类头\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "mixup_data() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m fold_results, class_names, all_fold_preds, all_fold_labels, all_fold_probs \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mrun_stratified_kfold_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 73\u001b[0m, in \u001b[0;36mrun_stratified_kfold_validation\u001b[1;34m(data_dir, num_classes, device, CFG)\u001b[0m\n\u001b[0;32m     70\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model(num_classes, device)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# 训练\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_two_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# 加载最优模型\u001b[39;00m\n\u001b[0;32m     76\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_path\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_best.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 31\u001b[0m, in \u001b[0;36mtrain_two_stage\u001b[1;34m(model, train_loader, val_loader, device, CFG, fold_num)\u001b[0m\n\u001b[0;32m     25\u001b[0m early_stopping1 \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[0;32m     26\u001b[0m     patience\u001b[38;5;241m=\u001b[39mCFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mearly_stopping_patience\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     27\u001b[0m     best_model_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_path\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_stage1.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage1_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m---> 31\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_mixup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muse_mixup\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmixup_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmixup_alpha\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate_epoch(model, val_loader, criterion, device)\n\u001b[0;32m     37\u001b[0m     history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_loader, criterion, optimizer, device, use_mixup, mixup_alpha)\u001b[0m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_mixup:\n\u001b[1;32m---> 17\u001b[0m     images, labels_a, labels_b, lam \u001b[38;5;241m=\u001b[39m \u001b[43mmixup_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmixup_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_mixup:\n",
      "\u001b[1;31mTypeError\u001b[0m: mixup_data() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "fold_results, class_names, all_fold_preds, all_fold_labels, all_fold_probs = \\\n",
    "    run_stratified_kfold_validation(data_dir, num_classes, device, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1376e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(fold_results, result_dir):\n",
    "    \"\"\"绘制所有fold的训练曲线和平均曲线\"\"\"\n",
    "    \n",
    "    num_folds = len(fold_results)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 单个fold的曲线 (第一个fold作为代表)\n",
    "    history = fold_results[0]['history']\n",
    "    epochs = np.arange(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss - 单fold\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Val', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch', fontsize=11)\n",
    "    axes[0, 0].set_ylabel('Loss', fontsize=11)\n",
    "    axes[0, 0].set_title(f'Loss Curve (Fold 1)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].legend(fontsize=10)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy - 单fold\n",
    "    axes[0, 1].plot(epochs, history['train_acc'], 'b-', label='Train', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, history['val_acc'], 'r-', label='Val', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch', fontsize=11)\n",
    "    axes[0, 1].set_ylabel('Accuracy', fontsize=11)\n",
    "    axes[0, 1].set_title(f'Accuracy Curve (Fold 1)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].legend(fontsize=10)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 平均Loss曲线\n",
    "    min_len = min([len(r['history']['train_loss']) for r in fold_results])\n",
    "    avg_train_loss = np.mean(\n",
    "        [r['history']['train_loss'][:min_len] for r in fold_results], axis=0\n",
    "    )\n",
    "    avg_val_loss = np.mean(\n",
    "        [r['history']['val_loss'][:min_len] for r in fold_results], axis=0\n",
    "    )\n",
    "    avg_epochs = np.arange(1, min_len + 1)\n",
    "    \n",
    "    axes[1, 0].plot(avg_epochs, avg_train_loss, 'b-', label='Train (Avg)', linewidth=2)\n",
    "    axes[1, 0].plot(avg_epochs, avg_val_loss, 'r-', label='Val (Avg)', linewidth=2)\n",
    "    axes[1, 0].fill_between(avg_epochs, avg_train_loss, alpha=0.2, color='blue')\n",
    "    axes[1, 0].fill_between(avg_epochs, avg_val_loss, alpha=0.2, color='red')\n",
    "    axes[1, 0].set_xlabel('Epoch', fontsize=11)\n",
    "    axes[1, 0].set_ylabel('Loss', fontsize=11)\n",
    "    axes[1, 0].set_title(f'Average Loss ({CFG[\"n_splits\"]} Folds)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].legend(fontsize=10)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 平均Accuracy曲线\n",
    "    avg_train_acc = np.mean(\n",
    "        [r['history']['train_acc'][:min_len] for r in fold_results], axis=0\n",
    "    )\n",
    "    avg_val_acc = np.mean(\n",
    "        [r['history']['val_acc'][:min_len] for r in fold_results], axis=0\n",
    "    )\n",
    "    \n",
    "    axes[1, 1].plot(avg_epochs, avg_train_acc, 'b-', label='Train (Avg)', linewidth=2)\n",
    "    axes[1, 1].plot(avg_epochs, avg_val_acc, 'r-', label='Val (Avg)', linewidth=2)\n",
    "    axes[1, 1].fill_between(avg_epochs, avg_train_acc, alpha=0.2, color='blue')\n",
    "    axes[1, 1].fill_between(avg_epochs, avg_val_acc, alpha=0.2, color='red')\n",
    "    axes[1, 1].set_xlabel('Epoch', fontsize=11)\n",
    "    axes[1, 1].set_ylabel('Accuracy', fontsize=11)\n",
    "    axes[1, 1].set_title(f'Average Accuracy ({CFG[\"n_splits\"]} Folds)', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].legend(fontsize=10)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(result_dir, 'training_curves.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\" 训练曲线已保存\")\n",
    "\n",
    "plot_training_curves(fold_results, result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085226b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_f1_metrics(fold_results, class_names, result_dir):\n",
    "    \"\"\"使用F1分数和详细的分类指标评估\"\"\"\n",
    "    \n",
    "    # 合并所有fold的预测\n",
    "    all_preds = np.concatenate([r['preds'] for r in fold_results])\n",
    "    all_labels = np.concatenate([r['labels'] for r in fold_results])\n",
    "    \n",
    "    # 逐fold的评估\n",
    "    print(\"\\n各Fold的F1分数:\")\n",
    "    print(\"-\" * 60)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for i, result in enumerate(fold_results):\n",
    "        preds = result['preds']\n",
    "        labels = result['labels']\n",
    "        \n",
    "        f1_weighted = f1_score(labels, preds, average='weighted')\n",
    "        f1_macro = f1_score(labels, preds, average='macro')\n",
    "        f1_micro = f1_score(labels, preds, average='micro')\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        \n",
    "        f1_scores.append(f1_weighted)\n",
    "        \n",
    "        print(f\"Fold {i+1}: Acc={acc:.4f}, F1_W={f1_weighted:.4f}, F1_M={f1_macro:.4f}\")\n",
    "    \n",
    "    mean_f1 = np.mean(f1_scores)\n",
    "    std_f1 = np.std(f1_scores)\n",
    "    \n",
    "    print(f\"\\n平均 Weighted F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "    \n",
    "    # 整体分类报告\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"整体分类报告（所有Fold合并）\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
    "    \n",
    "    # 按类别的F1分数\n",
    "    f1_per_class = f1_score(all_labels, all_preds, average=None)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    x = np.arange(len(class_names))\n",
    "    bars = ax.bar(x, f1_per_class, color='steelblue', alpha=0.8)\n",
    "    ax.set_xlabel('Class', fontsize=12)\n",
    "    ax.set_ylabel('F1 Score', fontsize=12)\n",
    "    ax.set_title('F1 Score per Class', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 添加数值标签\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(result_dir, 'f1_scores_per_class.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\n F1分数图已保存\")\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "all_preds, all_labels = evaluate_with_f1_metrics(fold_results, class_names, result_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd70f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(all_preds, all_labels, class_names, result_dir):\n",
    "    \"\"\"绘制混淆矩阵\"\"\"\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names,\n",
    "                cbar_kws={'label': 'Count'},\n",
    "                ax=ax, square=True)\n",
    "    \n",
    "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "    ax.set_ylabel('True Label', fontsize=12)\n",
    "    ax.set_title(f'Confusion Matrix ({CFG[\"n_splits\"]}-Fold CV)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(result_dir, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\" 混淆矩阵已保存\")\n",
    "    \n",
    "    # 计算每类的准确率\n",
    "    print(\"\\n各类别准确率:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_acc = cm[i, i] / cm[i].sum()\n",
    "        print(f\"{class_name:15s}: {class_acc*100:.2f}%\")\n",
    "\n",
    "plot_confusion_matrix(all_preds, all_labels, class_names, result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2ff2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_predictions(fold_results, class_names, result_dir, num_samples=12):\n",
    "    \"\"\"随机展示模型预测结果\"\"\"\n",
    "    \n",
    "    # 合并所有fold的数据\n",
    "    all_indices = []\n",
    "    for result in fold_results:\n",
    "        all_indices.append(result['val_idx'])\n",
    "    all_indices = np.concatenate(all_indices)\n",
    "    \n",
    "    # 加载完整数据集\n",
    "    full_dataset = datasets.ImageFolder(\n",
    "        root=data_dir,\n",
    "        transform=data_transforms[\"val\"]\n",
    "    )\n",
    "    \n",
    "    # 选择一个fold的模型进行推理\n",
    "    best_fold = np.argmax([r['accuracy'] for r in fold_results])\n",
    "    model_path = fold_results[best_fold]['model_path']\n",
    "    \n",
    "    model = create_model(num_classes, device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    # 从验证集中随机选择示例\n",
    "    val_indices = fold_results[best_fold]['val_idx']\n",
    "    random_sample_indices = np.random.choice(val_indices, size=min(num_samples, len(val_indices)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, sample_idx in enumerate(random_sample_indices):\n",
    "            image, label = full_dataset[sample_idx]\n",
    "            \n",
    "            # 推理\n",
    "            image_batch = image.unsqueeze(0).to(device)\n",
    "            output = model(image_batch)\n",
    "            prob = F.softmax(output, dim=1)\n",
    "            pred = prob.argmax(dim=1).item()\n",
    "            confidence = prob[0, pred].item()\n",
    "            \n",
    "            # 反标准化显示\n",
    "            image_np = image.cpu().numpy()\n",
    "            image_np = image_np * np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1) + \\\n",
    "                       np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n",
    "            image_np = np.clip(image_np, 0, 1)\n",
    "            image_np = np.transpose(image_np, (1, 2, 0))\n",
    "            \n",
    "            ax = axes[idx]\n",
    "            ax.imshow(image_np)\n",
    "            \n",
    "            true_label = class_names[label]\n",
    "            pred_label = class_names[pred]\n",
    "            color = 'green' if label == pred else 'red'\n",
    "            \n",
    "            title = f'True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.2f}'\n",
    "            ax.set_title(title, color=color, fontweight='bold', fontsize=10)\n",
    "            ax.axis('off')\n",
    "    \n",
    "    for i in range(len(random_sample_indices), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(result_dir, 'random_predictions.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"示例预测已保存（使用Fold {best_fold+1}的最优模型）\")\n",
    "\n",
    "visualize_random_predictions(fold_results, class_names, result_dir, num_samples=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db176c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(fold_results, device, num_iterations=100):\n",
    "    \"\"\"测量模型推理时间\"\"\"\n",
    "    \n",
    "    model_path = fold_results[0]['model_path']\n",
    "    model = create_model(num_classes, device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    # 生成虚拟输入\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    \n",
    "    # 预热GPU\n",
    "    with torch.no_grad():\n",
    "        for _ in range(10):\n",
    "            _ = model(dummy_input)\n",
    "    \n",
    "    # 测量推理时间\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_iterations):\n",
    "            _ = model(dummy_input)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    avg_time_per_image = (total_time / num_iterations) * 1000  # ms\n",
    "    fps = 1 / (total_time / num_iterations)\n",
    "    \n",
    "    print(f\"\\n推理时间统计 (n={num_iterations} iterations):\")\n",
    "    print(f\"  总时间: {total_time:.4f}s\")\n",
    "    print(f\"  单张图像: {avg_time_per_image:.4f}ms\")\n",
    "    print(f\"  FPS: {fps:.2f}\")\n",
    "    \n",
    "    # 不同batch size的推理时间\n",
    "    batch_sizes = [1, 4, 8, 16, 32]\n",
    "    fps_list = []\n",
    "    \n",
    "    print(f\"\\n不同Batch Size的推理速度:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        dummy_input = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(50):\n",
    "                _ = model(dummy_input)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        fps = (50 * batch_size) / elapsed\n",
    "        fps_list.append(fps)\n",
    "        print(f\"  Batch {batch_size:2d}: {fps:6.2f} FPS\")\n",
    "    \n",
    "    # 绘制速度-精度图\n",
    "    accuracies = [r['accuracy'] for r in fold_results]\n",
    "    mean_acc = np.mean(accuracies)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(batch_sizes)))\n",
    "    for i, (batch_size, fps) in enumerate(zip(batch_sizes, fps_list)):\n",
    "        ax.scatter(fps, mean_acc, s=300, alpha=0.7, color=colors[i], \n",
    "                  label=f'Batch {batch_size}', edgecolors='black', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('FPS (Inference Speed)', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax.set_title('Speed-Accuracy Trade-off (MobileNetV3-Large)', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim([mean_acc - 0.05, min(mean_acc + 0.05, 1.0)])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(result_dir, 'speed_accuracy_tradeoff.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\n 速度-精度图已保存\")\n",
    "    \n",
    "    return avg_time_per_image, fps\n",
    "\n",
    "inference_time, fps = measure_inference_time(fold_results, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c4ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    \"\"\"Grad-CAM实现\"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "        \n",
    "        # 注册hook\n",
    "        self.target_layer.register_forward_hook(self.save_activations)\n",
    "        self.target_layer.register_backward_hook(self.save_gradients)\n",
    "    \n",
    "    def save_activations(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "    \n",
    "    def save_gradients(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "    \n",
    "    def generate_cam(self, input_tensor, class_idx):\n",
    "        \"\"\"生成CAM\"\"\"\n",
    "        output = self.model(input_tensor)\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        loss = output[0, class_idx]\n",
    "        loss.backward()\n",
    "        \n",
    "        gradients = self.gradients.mean(dim=[2, 3], keepdim=True)\n",
    "        cam = (gradients * self.activations).sum(dim=1, keepdim=True)\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam.squeeze().cpu().numpy()\n",
    "        \n",
    "        # 归一化\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "        \n",
    "        return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aaa1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradcam_predictions(fold_results, class_names, result_dir, num_samples=6):\n",
    "    \"\"\"使用Grad-CAM可视化模型关注区域\"\"\"\n",
    "    \n",
    "    model_path = fold_results[0]['model_path']\n",
    "    model = create_model(num_classes, device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    # 获取features的最后一层\n",
    "    target_layer = model.features[-1]\n",
    "    grad_cam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # 加载验证集\n",
    "    full_dataset = datasets.ImageFolder(\n",
    "        root=data_dir,\n",
    "        transform=data_transforms[\"val\"]\n",
    "    )\n",
    "    \n",
    "    val_indices = fold_results[0]['val_idx']\n",
    "    random_indices = np.random.choice(val_indices, size=min(num_samples, len(val_indices)), replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for row, sample_idx in enumerate(random_indices):\n",
    "            image, label = full_dataset[sample_idx]\n",
    "            \n",
    "            # 推理\n",
    "            image_batch = image.unsqueeze(0).to(device)\n",
    "            output = model(image_batch)\n",
    "            prob = F.softmax(output, dim=1)\n",
    "            pred = prob.argmax(dim=1).item()\n",
    "            \n",
    "            # 生成CAM\n",
    "            cam = grad_cam.generate_cam(image_batch, pred)\n",
    "            \n",
    "            # 反标准化原图\n",
    "            image_np = image.cpu().numpy()\n",
    "            image_np = image_np * np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1) + \\\n",
    "                       np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n",
    "            image_np = np.clip(image_np, 0, 1)\n",
    "            image_np = np.transpose(image_np, (1, 2, 0))\n",
    "            \n",
    "            # 原图\n",
    "            axes[row, 0].imshow(image_np)\n",
    "            axes[row, 0].set_title(f'Original\\n{class_names[label]}', fontsize=10)\n",
    "            axes[row, 0].axis('off')\n",
    "            \n",
    "            # CAM热力图\n",
    "            axes[row, 1].imshow(image_np)\n",
    "            axes[row, 1].imshow(cv2.resize(cam, (224, 224)), cmap='jet', alpha=0.5)\n",
    "            axes[row, 1].set_title(f'Grad-CAM\\nPred: {class_names[pred]}', fontsize=10)\n",
    "            axes[row, 1].axis('off')\n",
    "            \n",
    "            # 叠加图\n",
    "            cam_resized = cv2.resize(cam, (224, 224))\n",
    "            cam_colored = plt.cm.jet(cam_resized)[:, :, :3]\n",
    "            blended = 0.7 * image_np + 0.3 * cam_colored\n",
    "            axes[row, 2].imshow(blended)\n",
    "            axes[row, 2].set_title(f'Blended', fontsize=10)\n",
    "            axes[row, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(result_dir, 'gradcam_visualization.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\" Grad-CAM可视化已保存\")\n",
    "\n",
    "visualize_gradcam_predictions(fold_results, class_names, result_dir, num_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06f2601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): Conv2dNormActivation(\n",
      "      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=960, out_features=1280, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=1280, out_features=8, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae55fdb8",
   "metadata": {},
   "source": [
    "old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec91b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_curves(train_losses, val_losses, train_accs, val_accs):\n",
    "    epochs = len(train_losses)\n",
    "\n",
    "    plt.figure(figsize=(14,5))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(range(epochs), train_losses, label=\"Train Loss\")\n",
    "    plt.plot(range(epochs), val_losses, label=\"Val Loss\")\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(range(epochs), train_accs, label=\"Train Acc\")\n",
    "    plt.plot(range(epochs), val_accs, label=\"Val Acc\")\n",
    "    plt.title(\"Training & Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e09c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, test_loader, class_names, device):\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            outputs = model(imgs)\n",
    "            preds = outputs.argmax(dim=1).cpu()\n",
    "\n",
    "            preds_list.extend(preds.numpy())\n",
    "            labels_list.extend(labels.numpy())\n",
    "\n",
    "    # 混淆矩阵\n",
    "    cm = confusion_matrix(labels_list, preds_list)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "    # 分类指标报告\n",
    "    print(\"\\n 分类指标报告（Classification Report）\")\n",
    "    print(classification_report(labels_list, preds_list, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "299f62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def plot_multiclass_roc(model, test_loader, class_names, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_score = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            y_true.extend(labels.numpy())\n",
    "            y_score.extend(outputs.cpu().numpy())\n",
    "\n",
    "    # One-vs-Rest\n",
    "    y_true_bin = label_binarize(y_true, classes=list(range(len(class_names))))\n",
    "    y_score = np.array(y_score)\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "\n",
    "    for i in range(len(class_names)):\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC={roc_auc:.2f})\")\n",
    "\n",
    "    plt.plot([0,1], [0,1], \"k--\")\n",
    "    plt.title(\"ROC Curves (One-vs-Rest)\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
